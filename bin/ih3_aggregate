#! python
import argparse, sys, os, re
os.environ['USE_PYGEOS'] = '0' 
import icesath3 as gh3
import dask, logging, glob, ast
from dask.distributed import Client, progress

def getCmdArgs():
    p = argparse.ArgumentParser(description = "Aggregate icesat data spatially using h3 (hexagons)")
  
    p.add_argument("-i", "--input", dest="input", required=True, type=str, help="input file or directory with indexed icesat data")
    p.add_argument("-o", "--output", dest="output", required=True, type=str, help="output directory or file name")
    p.add_argument("-m", "--mapper", dest="mapper", required=False, type=str, default='mean', help="how to aggregate data under pandas standard - string with single function name or quoted list/dictionary: 'mean' or \"['mean', 'std', 'count']\" or \"{'agbd':['mean','count'], 'sensitivity':['mean']}\"")    
    p.add_argument("-r", "--res", dest="res", required=False, type=int,  default=6, help="resolution level of h3 system to aggregate to")
    p.add_argument("-d", "--drop_columns", dest="drop_columns", required=False, nargs='+', default=['shot_number'], help="columns to ignore in aggregation - may be useful for non-dictionary mappers")
    p.add_argument("-u", "--use_columns", dest="use_columns", required=False, nargs='+', default=None, help="only aggregate few columns")

    p.add_argument("-g", "--img", dest="img", required=False, action='store_true', help="force image output - resamples hexagons if index is h3")
    p.add_argument("-t", "--tiles", dest="tiles", required=False, action='store_true', help="export tiles instead of merging outputs to single vector/img file")
    p.add_argument("-f", "--format", dest="format", required=False, type=str, default='parquet', help="output files format [default = parquet]")
    
    n = os.cpu_count() // 4
    p.add_argument("-n", "--cores", dest="cores", required=False, type=int, default=n, help=f"number of cpu cores to use [default = {n}]")
    p.add_argument("-s", "--threads", dest="threads", required=False, type=int, default=1, help="number of threads per cpu [default = 1]")
    p.add_argument("-A", "--ram", dest="ram", required=False, type=int, default=20, help="maximum RAM usage per cpu - in Giga Bytes [default = 20]")
    p.add_argument("-p", "--port", dest="port", required=False, type=int, default=30000, help="port where to open dask dashboard [default = 30000]")
    
    cmdargs = p.parse_args()
    return cmdargs

if __name__ == '__main__':    
    args = getCmdArgs()
    is_greedy = gh3.safe.tell_user(os.cpu_count() // 2, verbose=False)
    is_greedy = gh3.safe.tell_user(os.cpu_count() // 2) # 2nd time gets cpu percent correctly
    if is_greedy and not gh3.safe.clear_user(): 
        print("## -- too many resources used by your current spawned processes - try again once those are finished")
        sys.exit("## -- EXIT")
    else:
        print("## -- user cleared - opening distributed backend")
    
    client = Client(n_workers=args.cores, threads_per_worker=args.threads, dashboard_address=f':{args.port}', memory_limit=f'{args.ram}GB', silence_logs=logging.ERROR)
    print("## -- dask dashboard available at:", client.dashboard_link)
    
    cols = args.use_columns
    f_list = glob.glob(args.input + ('' if os.path.isfile(args.input) else '/*'))
    f = f_list[0]
    is_parquet = f.endswith('.parquet') or f.endswith('.pq') or f.endswith('.parq')
    if cols is None:        
        cols = gh3.read_parquet_schema(f) if is_parquet else gh3.read_geopackage_schema(f)        
        cols = cols[~cols.column.isin(args.drop_columns) & (cols.dtype != 'string') & (cols.dtype != 'str')].column.to_list()    
    cols = [i for i in cols if i != 'geometry']
    
    print("## -- reading distributed data")
    if is_parquet:
        gdf = dask.dataframe.read_parquet(args.input, columns=cols)
    else:
        gdf = dask.dataframe.from_delayed([gh3.read_gpkg(i, cols) for i in f_list])
            
    print("## -- scheduling aggregation")
    mapper = ast.literal_eval(args.mapper) if '[' in args.mapper or "{" in args.mapper else args.mapper
    fagg = gh3.h3_aggregate 
    
    aggdf = fagg(gdf, level=args.res, agg=mapper, centroids=False)
    cols = [i for i in aggdf.columns if 'h3' not in i and 'egi' not in i]
    aggdf = aggdf[cols]
       
    if args.img:
        print("## -- scheduling raster transformation")
        args.format = 'tif'
        aggdf = gh3.raster_parts(aggdf)
    
    print("## -- loading and exporting outputs")
    out = args.output
    out = re.sub('/*$', '', out)
    if args.tiles:
        files = gh3.export_parts(aggdf, out, fmt=args.format)
        files = files.persist()
        progress(files)
        print('')
    elif args.img:
        ras = gh3.compute_raster(aggdf, show_progress=True)
        if ".tif" not in out:
            out += ".tif"
        ras.rio.to_raster(out)
        olzw = out.replace('.tif', '_lzw.tif')
        cmd = f"gdal_translate -co BIGTIFF=YES -co compress=LZW -co TILED=YES -co BLOCKXSIZE=256 -co BLOCKYSIZE=256 -ot Float32 {out} {olzw}"
        os.system(cmd)
        os.unlink(out)
        os.system(f"mv {olzw} {out}")
    else:
        aggdf = aggdf.persist()
        progress(aggdf)
        print('')
                
        gpkg = aggdf.compute()
        if f".{args.format}" not in out:
            out += f".{args.format}"
        if args.format in ['parq', 'parquet', 'pq']:
            gpkg.to_parquet(out)
        else:
            gpkg.to_file(out)
    
    try:
        client.close()
    except:
        pass

    sys.exit("## -- DONE")